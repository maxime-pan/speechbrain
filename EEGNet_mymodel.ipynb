{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxime-pan/speechbrain/blob/main/EEGNet_mymodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SpeechBrain-MOABB: EEGNet"
      ],
      "metadata": {
        "id": "gxG3UuKZrsW6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnRSS3jGxVsk"
      },
      "source": [
        "## **Prerequisites**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download SpeechBrain-MOABB\n",
        "\n",
        "SpeechBrain-MOABB can be downloaded from the GitHub repository listed below."
      ],
      "metadata": {
        "id": "-GhrvkOFT9_t"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSOTkqPsJXxZ"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/speechbrain/benchmarks\n",
        "%cd /content/benchmarks\n",
        "!git checkout eeg\n",
        "\n",
        "%cd /content/benchmarks/benchmarks/MOABB\n",
        "!pip install -r extra-requirements.txt # Install additional dependencies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxr8nZ7bbP15"
      },
      "source": [
        "### Install SpeechBrain and SpeechBrain-MOABB requirements, and install SpeechBrain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFEX28mu4dFt"
      },
      "source": [
        "%%capture\n",
        "# Clone SpeechBrain repository (development branch)\n",
        "%cd /content/\n",
        "!git clone https://github.com/speechbrain/speechbrain/\n",
        "%cd /content/speechbrain/\n",
        "\n",
        "# Install required dependencies\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Install SpeechBrain in editable mode\n",
        "!pip install -e .\n",
        "\n",
        "%cd /content/\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create mymodel.py"
      ],
      "metadata": {
        "id": "E3BebVYqaAsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/benchmarks/benchmarks/MOABB/models/mymodel.py\n",
        "\"\"\"EEGNet from https://doi.org/10.1088/1741-2552/aace8c.\n",
        "Shallow and lightweight convolutional neural network proposed for a general decoding of single-trial EEG signals.\n",
        "It was proposed for P300, error-related negativity, motor execution, motor imagery decoding.\n",
        "\n",
        "Authors\n",
        " * Davide Borra, 2021\n",
        "\"\"\"\n",
        "import torch\n",
        "import speechbrain as sb\n",
        "\n",
        "\n",
        "class EEGNet(torch.nn.Module):\n",
        "    \"\"\"EEGNet.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    input_shape: tuple\n",
        "        The shape of the input.\n",
        "    cnn_temporal_kernels: int\n",
        "        Number of kernels in the 2d temporal convolution.\n",
        "    cnn_temporal_kernelsize: tuple\n",
        "        Kernel size of the 2d temporal convolution.\n",
        "    cnn_spatial_depth_multiplier: int\n",
        "        Depth multiplier of the 2d spatial depthwise convolution.\n",
        "    cnn_spatial_max_norm: float\n",
        "        Kernel max norm of the 2d spatial depthwise convolution.\n",
        "    cnn_spatial_pool: tuple\n",
        "        Pool size and stride after the 2d spatial depthwise convolution.\n",
        "    cnn_septemporal_depth_multiplier: int\n",
        "        Depth multiplier of the 2d temporal separable convolution.\n",
        "    cnn_septemporal_kernelsize: tuple\n",
        "        Kernel size of the 2d temporal separable convolution.\n",
        "    cnn_septemporal_pool: tuple\n",
        "        Pool size and stride after the 2d temporal separable convolution.\n",
        "    cnn_pool_type: string\n",
        "        Pooling type.\n",
        "    dropout: float\n",
        "        Dropout probability.\n",
        "    dense_max_norm: float\n",
        "        Weight max norm of the fully-connected layer.\n",
        "    dense_n_neurons: int\n",
        "        Number of output neurons.\n",
        "    activation_type: str\n",
        "        Activation function of the hidden layers.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    #>>> inp_tensor = torch.rand([1, 200, 32, 1])\n",
        "    #>>> model = EEGNet(input_shape=inp_tensor.shape)\n",
        "    #>>> output = model(inp_tensor)\n",
        "    #>>> output.shape\n",
        "    #torch.Size([1,4])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape=None,  # (1, T, C, 1)\n",
        "        cnn_temporal_kernels=8,\n",
        "        cnn_temporal_kernelsize=(33, 1),\n",
        "        cnn_spatial_depth_multiplier=2,\n",
        "        cnn_spatial_max_norm=1.0,\n",
        "        cnn_spatial_pool=(4, 1),\n",
        "        cnn_septemporal_depth_multiplier=1,\n",
        "        cnn_septemporal_point_kernels=None,\n",
        "        cnn_septemporal_kernelsize=(17, 1),\n",
        "        cnn_septemporal_pool=(8, 1),\n",
        "        cnn_pool_type=\"avg\",\n",
        "        dropout=0.5,\n",
        "        dense_max_norm=0.25,\n",
        "        dense_n_neurons=4,\n",
        "        activation_type=\"elu\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if input_shape is None:\n",
        "            raise ValueError(\"Must specify input_shape\")\n",
        "        if activation_type == \"gelu\":\n",
        "            activation = torch.nn.GELU()\n",
        "        elif activation_type == \"elu\":\n",
        "            activation = torch.nn.ELU()\n",
        "        elif activation_type == \"relu\":\n",
        "            activation = torch.nn.ReLU()\n",
        "        elif activation_type == \"leaky_relu\":\n",
        "            activation = torch.nn.LeakyReLU()\n",
        "        elif activation_type == \"prelu\":\n",
        "            activation = torch.nn.PReLU()\n",
        "        else:\n",
        "            raise ValueError(\"Wrong hidden activation function\")\n",
        "        self.default_sf = 128  # sampling rate of the original publication (Hz)\n",
        "        # T = input_shape[1]\n",
        "        C = input_shape[2]\n",
        "\n",
        "        # CONVOLUTIONAL MODULE\n",
        "        self.conv_module = torch.nn.Sequential()\n",
        "        # Temporal convolution\n",
        "        self.conv_module.add_module(\n",
        "            \"conv_0\",\n",
        "            sb.nnet.CNN.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=cnn_temporal_kernels,\n",
        "                kernel_size=cnn_temporal_kernelsize,\n",
        "                padding=\"same\",\n",
        "                padding_mode=\"constant\",\n",
        "                bias=False,\n",
        "                swap=True,\n",
        "            ),\n",
        "        )\n",
        "        self.conv_module.add_module(\n",
        "            \"bnorm_0\",\n",
        "            sb.nnet.normalization.BatchNorm2d(\n",
        "                input_size=cnn_temporal_kernels, momentum=0.01, affine=True,\n",
        "            ),\n",
        "        )\n",
        "        self.conv_module.add_module(\n",
        "            \"pool_0\",\n",
        "            sb.nnet.pooling.Pooling2d(\n",
        "                pool_type=cnn_pool_type,\n",
        "                kernel_size=cnn_spatial_pool,\n",
        "                stride=cnn_spatial_pool,\n",
        "                pool_axis=[1, 2],\n",
        "            ),\n",
        "        )\n",
        "        # Spatial depthwise convolution\n",
        "        cnn_spatial_kernels = (\n",
        "            cnn_spatial_depth_multiplier * cnn_temporal_kernels\n",
        "        )\n",
        "        self.conv_module.add_module(\n",
        "            \"conv_1\",\n",
        "            sb.nnet.CNN.Conv2d(\n",
        "                in_channels=cnn_temporal_kernels,\n",
        "                out_channels=cnn_spatial_kernels,\n",
        "                kernel_size=(1, C),\n",
        "                groups=cnn_temporal_kernels,\n",
        "                padding=\"valid\",\n",
        "                bias=False,\n",
        "                max_norm=cnn_spatial_max_norm,\n",
        "                swap=True,\n",
        "            ),\n",
        "        )\n",
        "        self.conv_module.add_module(\n",
        "            \"bnorm_1\",\n",
        "            sb.nnet.normalization.BatchNorm2d(\n",
        "                input_size=cnn_spatial_kernels, momentum=0.01, affine=True,\n",
        "            ),\n",
        "        )\n",
        "        self.conv_module.add_module(\"act_1\", activation)\n",
        "        self.conv_module.add_module(\n",
        "            \"pool_1\",\n",
        "            sb.nnet.pooling.Pooling2d(\n",
        "                pool_type=cnn_pool_type,\n",
        "                kernel_size=cnn_spatial_pool,\n",
        "                stride=cnn_spatial_pool,\n",
        "                pool_axis=[1, 2],\n",
        "            ),\n",
        "        )\n",
        "        self.conv_module.add_module(\"dropout_1\", torch.nn.Dropout(p=dropout))\n",
        "\n",
        "        # Temporal separable convolution\n",
        "        cnn_septemporal_kernels = (\n",
        "            cnn_spatial_kernels * cnn_septemporal_depth_multiplier\n",
        "        )\n",
        "        self.conv_module.add_module(\n",
        "            \"conv_2\",\n",
        "            sb.nnet.CNN.Conv2d(\n",
        "                in_channels=cnn_spatial_kernels,\n",
        "                out_channels=cnn_septemporal_kernels,\n",
        "                kernel_size=cnn_septemporal_kernelsize,\n",
        "                groups=cnn_spatial_kernels,\n",
        "                padding=\"same\",\n",
        "                padding_mode=\"constant\",\n",
        "                bias=False,\n",
        "                swap=True,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if cnn_septemporal_point_kernels is None:\n",
        "            cnn_septemporal_point_kernels = cnn_septemporal_kernels\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"conv_3\",\n",
        "            sb.nnet.CNN.Conv2d(\n",
        "                in_channels=cnn_septemporal_kernels,\n",
        "                out_channels=cnn_septemporal_point_kernels,\n",
        "                kernel_size=(1, 1),\n",
        "                padding=\"valid\",\n",
        "                bias=False,\n",
        "                swap=True,\n",
        "            ),\n",
        "        )\n",
        "        self.conv_module.add_module(\n",
        "            \"bnorm_3\",\n",
        "            sb.nnet.normalization.BatchNorm2d(\n",
        "                input_size=cnn_septemporal_point_kernels,\n",
        "                momentum=0.01,\n",
        "                affine=True,\n",
        "            ),\n",
        "        )\n",
        "        self.conv_module.add_module(\"act_3\", activation)\n",
        "        self.conv_module.add_module(\n",
        "            \"pool_3\",\n",
        "            sb.nnet.pooling.Pooling2d(\n",
        "                pool_type=cnn_pool_type,\n",
        "                kernel_size=cnn_septemporal_pool,\n",
        "                stride=cnn_septemporal_pool,\n",
        "                pool_axis=[1, 2],\n",
        "            ),\n",
        "        )\n",
        "        self.conv_module.add_module(\"dropout_3\", torch.nn.Dropout(p=dropout))\n",
        "\n",
        "        # Shape of intermediate feature maps\n",
        "        out = self.conv_module(\n",
        "            torch.ones((1,) + tuple(input_shape[1:-1]) + (1,))\n",
        "        )\n",
        "        dense_input_size = self._num_flat_features(out)\n",
        "        # DENSE MODULE\n",
        "        self.dense_module = torch.nn.Sequential()\n",
        "        self.dense_module.add_module(\n",
        "            \"flatten\", torch.nn.Flatten(),\n",
        "        )\n",
        "        self.dense_module.add_module(\n",
        "            \"fc_out\",\n",
        "            sb.nnet.linear.Linear(\n",
        "                input_size=dense_input_size,\n",
        "                n_neurons=dense_n_neurons,\n",
        "                max_norm=dense_max_norm,\n",
        "            ),\n",
        "        )\n",
        "        self.dense_module.add_module(\"act_out\", torch.nn.LogSoftmax(dim=1))\n",
        "\n",
        "    def _num_flat_features(self, x):\n",
        "        \"\"\"Returns the number of flattened features from a tensor.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor\n",
        "            Input feature map.\n",
        "        \"\"\"\n",
        "\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns the output of the model.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor (batch, time, EEG channel, channel)\n",
        "            Input to convolve. 4d tensors are expected.\n",
        "        \"\"\"\n",
        "        x = self.conv_module(x)\n",
        "        x = self.dense_module(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "PqGwJ7xrVIjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define a yaml file containing the hyper-parameters defining a decoding pipeline**\n",
        "\n",
        "Let us address 4-class motor imagery decoding using BNCI2014-001 dataset (also known as \"BCI IV2a dataset\"), by adopting a leave-one-session-out strategy using the first participants' signals, leaving out the session named '0test'. EEGNet is used as decoder.\n",
        "\n",
        "You can set all hyper-parameters to specific values if you already know them; otherwise, you can set them to placeholders (i.e., as `!PLACEHOLDER`). For example, folders (e.g., the data folder, the folder for compressed dataset, and the output folder), and dataset information (e.g., data iterator, index of subject and session to use) are usually kept as placeholders.\n",
        "\n",
        "Before start writing the yaml file, please follow the SpeechBrain tutorial dedicated to HyperPyYAML at https://speechbrain.github.io/tutorial_basics.html."
      ],
      "metadata": {
        "id": "f45uzf3mUNyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperpyyaml import load_hyperpyyaml, dump_hyperpyyaml\n",
        "\n",
        "example_hyperparams = \"\"\"\n",
        "seed: 1234\n",
        "__set_torchseed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# DIRECTORIES\n",
        "data_folder: !PLACEHOLDER  #'/path/to/dataset'. The dataset will be automatically downloaded in this folder\n",
        "cached_data_folder: !PLACEHOLDER #'path/to/pickled/dataset'\n",
        "output_folder: !PLACEHOLDER #'path/to/results'\n",
        "\n",
        "# DATASET HPARS\n",
        "# Defining the MOABB dataset.\n",
        "dataset: !new:moabb.datasets.BNCI2014001\n",
        "save_prepared_dataset: True # set to True if you want to save the prepared dataset as a pkl file to load and use afterwards\n",
        "data_iterator_name: !PLACEHOLDER\n",
        "target_subject_idx: !PLACEHOLDER\n",
        "target_session_idx: !PLACEHOLDER\n",
        "events_to_load: null # all events will be loaded\n",
        "original_sample_rate: 250 # Original sampling rate provided by dataset authors\n",
        "sample_rate: 125 # Target sampling rate (Hz)\n",
        "# band-pass filtering cut-off frequencies\n",
        "fmin: 0.13 # @orion_step1: --fmin~\"uniform(0.1, 5, precision=2)\"\n",
        "fmax: 46.0 # @orion_step1: --fmax~\"uniform(20.0, 50.0, precision=3)\"\n",
        "n_classes: 4\n",
        "# tmin, tmax respect to stimulus onset that define the interval attribute of the dataset class\n",
        "# trial begins (0 s), cue (2 s, 1.25 s long); each trial is 6 s long\n",
        "# dataset interval starts from 2\n",
        "# -->tmin tmax are referred to this start value (e.g., tmin=0.5 corresponds to 2.5 s)\n",
        "tmin: 0.\n",
        "tmax: 4.0 # @orion_step1: --tmax~\"uniform(1.0, 4.0, precision=2)\"\n",
        "# number of steps used when selecting adjacent channels from a seed channel (default at Cz)\n",
        "n_steps_channel_selection: 2 # @orion_step1: --n_steps_channel_selection~\"uniform(1, 3,discrete=True)\"\n",
        "T: !apply:math.ceil\n",
        "    - !ref <sample_rate> * (<tmax> - <tmin>)\n",
        "C: 22\n",
        "# We here specify how to perfom test:\n",
        "# - If test_with: 'last' we perform test with the latest model.\n",
        "# - if test_with: 'best, we perform test with the best model (according to the metric specified in test_key)\n",
        "# The variable avg_models can be used to average the parameters of the last (or best) N saved models before testing.\n",
        "# This can have a regularization effect. If avg_models: 1, the last (or best) model is used directly.\n",
        "test_with: 'last' # 'last' or 'best'\n",
        "test_key: \"acc\" # Possible opts: \"loss\", \"f1\", \"auc\", \"acc\"\n",
        "\n",
        "# METRICS\n",
        "f1: !name:sklearn.metrics.f1_score\n",
        "    average: 'macro'\n",
        "acc: !name:sklearn.metrics.balanced_accuracy_score\n",
        "cm: !name:sklearn.metrics.confusion_matrix\n",
        "metrics:\n",
        "    f1: !ref <f1>\n",
        "    acc: !ref <acc>\n",
        "    cm: !ref <cm>\n",
        "# TRAINING HPARS\n",
        "n_train_examples: 100  # it will be replaced in the train script\n",
        "# checkpoints to average\n",
        "avg_models: 10 # @orion_step1: --avg_models~\"uniform(1, 15,discrete=True)\"\n",
        "number_of_epochs: 862 # @orion_step1: --number_of_epochs~\"uniform(250, 1000, discrete=True)\"\n",
        "lr: 0.0001 # @orion_step1: --lr~\"choices([0.01, 0.005, 0.001, 0.0005, 0.0001])\"\n",
        "# Learning rate scheduling (cyclic learning rate is used here)\n",
        "max_lr: !ref <lr> # Upper bound of the cycle (max value of the lr)\n",
        "base_lr: 0.00000001 # Lower bound in the cycle (min value of the lr)\n",
        "step_size_multiplier: 5 #from 2 to 8\n",
        "step_size: !apply:round\n",
        "    - !ref <step_size_multiplier> * <n_train_examples> / <batch_size>\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n",
        "    base_lr: !ref <base_lr>\n",
        "    max_lr: !ref <max_lr>\n",
        "    step_size: !ref <step_size>\n",
        "label_smoothing: 0.0\n",
        "loss: !name:speechbrain.nnet.losses.nll_loss\n",
        "    label_smoothing: !ref <label_smoothing>\n",
        "optimizer: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter  # epoch counter\n",
        "    limit: !ref <number_of_epochs>\n",
        "batch_size_exponent: 4 # @orion_step1: --batch_size_exponent~\"uniform(4, 6,discrete=True)\"\n",
        "batch_size: !ref 2 ** <batch_size_exponent>\n",
        "valid_ratio: 0.2\n",
        "\n",
        "# DATA AUGMENTATION\n",
        "# cutcat (disabled when min_num_segments=max_num_segments=1)\n",
        "max_num_segments: 3 # @orion_step2: --max_num_segments~\"uniform(2, 6, discrete=True)\"\n",
        "cutcat: !new:speechbrain.augment.time_domain.CutCat\n",
        "    min_num_segments: 2\n",
        "    max_num_segments: !ref <max_num_segments>\n",
        "# random amplitude gain between 0.5-1.5 uV (disabled when amp_delta=0.)\n",
        "amp_delta: 0.01742 # @orion_step2: --amp_delta~\"uniform(0.0, 0.5)\"\n",
        "rand_amp: !new:speechbrain.augment.time_domain.RandAmp\n",
        "    amp_low: !ref 1 - <amp_delta>\n",
        "    amp_high: !ref 1 + <amp_delta>\n",
        "# random shifts between -300 ms to 300 ms (disabled when shift_delta=0.)\n",
        "shift_delta_: 1 # orion_step2: --shift_delta_~\"uniform(0, 25, discrete=True)\"\n",
        "shift_delta: !ref 1e-2 * <shift_delta_> # 0.250 # 0.-0.25 with steps of 0.01\n",
        "min_shift: !apply:math.floor\n",
        "    - !ref 0 - <sample_rate> * <shift_delta>\n",
        "max_shift: !apply:math.floor\n",
        "    - !ref 0 + <sample_rate> * <shift_delta>\n",
        "time_shift: !new:speechbrain.augment.freq_domain.RandomShift\n",
        "    min_shift: !ref <min_shift>\n",
        "    max_shift: !ref <max_shift>\n",
        "    dim: 1\n",
        "# injection of gaussian white noise\n",
        "snr_white_low: 15.0 # @orion_step2: --snr_white_low~\"uniform(0.0, 15, precision=2)\"\n",
        "snr_white_delta: 19.1 # @orion_step2: --snr_white_delta~\"uniform(5.0, 20.0, precision=3)\"\n",
        "snr_white_high: !ref <snr_white_low> + <snr_white_delta>\n",
        "add_noise_white: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    snr_low: !ref <snr_white_low>\n",
        "    snr_high: !ref <snr_white_high>\n",
        "\n",
        "repeat_augment: 1 # @orion_step1: --repeat_augment 0\n",
        "augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    parallel_augment_fixed_bs: True\n",
        "    repeat_augment: !ref <repeat_augment>\n",
        "    shuffle_augmentations: True\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augmentations: [\n",
        "        !ref <cutcat>,\n",
        "        !ref <rand_amp>,\n",
        "        !ref <time_shift>,\n",
        "        !ref <add_noise_white>]\n",
        "\n",
        "# DATA NORMALIZATION\n",
        "dims_to_normalize: 1 # 1 (time) or 2 (EEG channels)\n",
        "normalize: !name:speechbrain.processing.signal_processing.mean_std_norm\n",
        "    dims: !ref <dims_to_normalize>\n",
        "# MODEL\n",
        "input_shape: [null, !ref <T>, !ref <C>, null]\n",
        "cnn_temporal_kernels: 61 # @orion_step1: --cnn_temporal_kernels~\"uniform(4, 64,discrete=True)\"\n",
        "cnn_temporal_kernelsize: 51 # @orion_step1: --cnn_temporal_kernelsize~\"uniform(24, 62,discrete=True)\"\n",
        "# depth multiplier for the spatial depthwise conv. layer\n",
        "cnn_spatial_depth_multiplier: 4 # @orion_step1: --cnn_spatial_depth_multiplier~\"uniform(1, 4,discrete=True)\"\n",
        "cnn_spatial_max_norm: 1.  # kernel max-norm constaint of the spatial depthwise conv. layer\n",
        "cnn_spatial_pool: 4\n",
        "cnn_septemporal_depth_multiplier: 1  # depth multiplier for the separable temporal conv. layer\n",
        "cnn_septemporal_point_kernels_ratio_: 7 # @orion_step1: --cnn_septemporal_point_kernels_ratio_~\"uniform(0, 8, discrete=True)\"\n",
        "cnn_septemporal_point_kernels_ratio: !ref <cnn_septemporal_point_kernels_ratio_> / 4\n",
        "## number of temporal filters in the separable temporal conv. layer\n",
        "cnn_septemporal_point_kernels_: !ref <cnn_temporal_kernels> * <cnn_spatial_depth_multiplier> * <cnn_septemporal_depth_multiplier>\n",
        "cnn_septemporal_point_kernels: !apply:math.ceil\n",
        "    - !ref <cnn_septemporal_point_kernels_ratio> * <cnn_septemporal_point_kernels_> + 1\n",
        "cnn_septemporal_kernelsize_: 15 # @orion_step1: --cnn_septemporal_kernelsize_~\"uniform(3, 24,discrete=True)\"\n",
        "max_cnn_spatial_pool: 4\n",
        "cnn_septemporal_kernelsize: !apply:round\n",
        "    - !ref <cnn_septemporal_kernelsize_> * <max_cnn_spatial_pool> / <cnn_spatial_pool>\n",
        "cnn_septemporal_pool: 7 # @orion_step1: --cnn_septemporal_pool~\"uniform(1, 8,discrete=True)\"\n",
        "cnn_pool_type: 'avg'\n",
        "dense_max_norm: 0.25  # kernel max-norm constaint of the dense layer\n",
        "dropout: 0.008464 # @orion_step1: --dropout~\"uniform(0.0, 0.5)\"\n",
        "activation_type: 'elu'\n",
        "\n",
        "model: !new:models.mymodel.EEGNet\n",
        "    input_shape: !ref <input_shape>\n",
        "    cnn_temporal_kernels: !ref <cnn_temporal_kernels>\n",
        "    cnn_temporal_kernelsize: [!ref <cnn_temporal_kernelsize>, 1]\n",
        "    cnn_spatial_depth_multiplier: !ref <cnn_spatial_depth_multiplier>\n",
        "    cnn_spatial_max_norm: !ref <cnn_spatial_max_norm>\n",
        "    cnn_spatial_pool: [!ref <cnn_spatial_pool>, 1]\n",
        "    cnn_septemporal_depth_multiplier: !ref <cnn_septemporal_depth_multiplier>\n",
        "    cnn_septemporal_point_kernels: !ref <cnn_septemporal_point_kernels>\n",
        "    cnn_septemporal_kernelsize: [!ref <cnn_septemporal_kernelsize>, 1]\n",
        "    cnn_septemporal_pool: [!ref <cnn_septemporal_pool>, 1]\n",
        "    cnn_pool_type: !ref <cnn_pool_type>\n",
        "    activation_type: !ref <activation_type>\n",
        "    dense_max_norm: !ref <dense_max_norm>\n",
        "    dropout: !ref <dropout>\n",
        "    dense_n_neurons: !ref <n_classes>\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "f_m7D3eiUbHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the yaml file on disk\n",
        "f = open('/content/mymodel_hyperparams.yaml', \"w\")\n",
        "f.write(example_hyperparams)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "DbJcCq8fYgL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Note about data augmentation**\n",
        "\n",
        "It is worth highlighting that in the previous yaml file, no data augmentation was included. However, you can easily add data augmentation by defining each augmenter (e.g., applying CutCat and random time shift).\n",
        "\n",
        "The so-defined augmenters are provided as input to the `Augmenter` class, that will combine and apply the augmenters. For instance, you can perform the augmenters in sequence or in parallel (`parallel_augment` input parameter), use one or more augmenters for augmenting each mini-batch of data (`min_augmentations` and `max_augmentations` input parameters), and repeat data augmentation multiple times for each mini-batch (`repeat_augment` input parameter). See `Augmenter` documentation for further details."
      ],
      "metadata": {
        "id": "mGsMSB2RvZX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation_hyperparams = \"\"\"\n",
        "# DATA AUGMENTATION\n",
        "# cutcat (disabled when min_num_segments=max_num_segments=1)\n",
        "max_num_segments: 3 # @orion_step2: --max_num_segments~\"uniform(2, 6, discrete=True)\"\n",
        "cutcat: !new:speechbrain.augment.time_domain.CutCat\n",
        "    min_num_segments: 2\n",
        "    max_num_segments: !ref <max_num_segments>\n",
        "# random amplitude gain between 0.5-1.5 uV (disabled when amp_delta=0.)\n",
        "amp_delta: 0.01742 # @orion_step2: --amp_delta~\"uniform(0.0, 0.5)\"\n",
        "rand_amp: !new:speechbrain.augment.time_domain.RandAmp\n",
        "    amp_low: !ref 1 - <amp_delta>\n",
        "    amp_high: !ref 1 + <amp_delta>\n",
        "# random shifts between -300 ms to 300 ms (disabled when shift_delta=0.)\n",
        "shift_delta_: 1 # orion_step2: --shift_delta_~\"uniform(0, 25, discrete=True)\"\n",
        "shift_delta: !ref 1e-2 * <shift_delta_> # 0.250 # 0.-0.25 with steps of 0.01\n",
        "min_shift: !apply:math.floor\n",
        "    - !ref 0 - <sample_rate> * <shift_delta>\n",
        "max_shift: !apply:math.floor\n",
        "    - !ref 0 + <sample_rate> * <shift_delta>\n",
        "time_shift: !new:speechbrain.augment.freq_domain.RandomShift\n",
        "    min_shift: !ref <min_shift>\n",
        "    max_shift: !ref <max_shift>\n",
        "    dim: 1\n",
        "# injection of gaussian white noise\n",
        "snr_white_low: 15.0 # @orion_step2: --snr_white_low~\"uniform(0.0, 15, precision=2)\"\n",
        "snr_white_delta: 19.1 # @orion_step2: --snr_white_delta~\"uniform(5.0, 20.0, precision=3)\"\n",
        "snr_white_high: !ref <snr_white_low> + <snr_white_delta>\n",
        "add_noise_white: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    snr_low: !ref <snr_white_low>\n",
        "    snr_high: !ref <snr_white_high>\n",
        "\n",
        "repeat_augment: 1 # @orion_step1: --repeat_augment 0\n",
        "augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    parallel_augment_fixed_bs: True\n",
        "    repeat_augment: !ref <repeat_augment>\n",
        "    shuffle_augmentations: True\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augmentations: [\n",
        "        !ref <cutcat>,\n",
        "        !ref <rand_amp>,\n",
        "        !ref <time_shift>,\n",
        "        !ref <add_noise_white>]\n",
        "\"\"\"\n",
        "\n",
        "example_hyperparams += data_augmentation_hyperparams"
      ],
      "metadata": {
        "id": "PtMD8LoevYg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train the neural network on a single cross-validation fold**\n",
        "\n",
        "Start network training by running the `train.py` script providing the filepath to the yaml file and by overriding the variables set as placeholders in the yaml file. Furthermore, here for brevity we override also the number of training epochs to 50 epochs (instead of 1000 epochs) with `--number_of_epochs 50`. It is worth highlighting that you can also override any other hyper-parameters in the same way."
      ],
      "metadata": {
        "id": "qiOA1AlVg48b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/benchmarks/benchmarks/MOABB/\n",
        "\n",
        "!python train.py /content/mymodel_hyperparams.yaml \\\n",
        "--data_folder '/content/data/BNCI2014001' \\\n",
        "--cached_data_folder '/content/data' \\\n",
        "--output_folder '/content/results/single-fold-example/BNCI2014001' \\\n",
        "--data_iterator_name 'leave-one-session-out' \\\n",
        "--target_subject_idx 0 \\\n",
        "--target_session_idx 1 \\\n",
        "--number_of_epochs 50 \\\n",
        "--device 'cpu' # Switch to cuda for a speed up.\n"
      ],
      "metadata": {
        "id": "pYEa8oubbvk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run a complete experiment by looping over the entire dataset**\n",
        "\n",
        "In the previous cell, `train.py` was called for a single cross-validation fold (e.g., one participant and one held-out session in leave-one-session-out cross-validation). Thus, we provide a command line interface for easily running training on all participants and cross-validation folds (using `./run_experiments.sh`).\n",
        "\n",
        "Here, besides the relevant folders, you should specify the hyper-parameter file, the number of participants and sessions to use, the data iteration scheme (leave-one-session-out or leave-one-subject-out). In addition, you can also run the code multiple times, each time with a different random seed used for initializing weights (by setting the `nruns` parameters). Finally, you can define the `device` to use (set to `cpu` if you do not have a GPU).\n",
        "\n"
      ],
      "metadata": {
        "id": "ITKUTG0JUcha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./run_experiments.sh --hparams /content/mymodel_hyperparams.yaml \\\n",
        "--data_folder '/content/data/BNCI2014001'\\\n",
        "--cached_data_folder '/content/data' \\\n",
        "--output_folder '/content/results/full-experiment/BNCI2014001' \\\n",
        "--nsbj 9 --nsess 2 --nruns 10 --train_mode 'leave-one-session-out' \\\n",
        "--number_of_epochs 100 \\\n",
        "--device 'cpu'"
      ],
      "metadata": {
        "id": "mOD1JbH2TSZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "--CL48ZxiU24"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}